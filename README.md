# AI-Powered Cloud Masking for Satellite Imagery

![Project Banner](media/image1.png)  
_Cloud masking visualization example_

## ğŸ“Œ Project Overview

This repository contains an end-to-end solution for **automated cloud detection in satellite imagery** using machine learning. The system processes multi-spectral satellite data to generate accurate cloud masks, addressing a critical challenge in remote sensing applications.

## ğŸš€ Key Features

- **Multi-model Architecture**: Implements both deep learning (UNet, DeepLabV3+) and classical (Random Forest) approaches
- **Advanced Preprocessing**: Handles 4-band satellite imagery with specialized normalization
- **Competition-Ready**: Fully compliant with Kaggle-style submission requirements
- **Reproducible**: Docker support and detailed experiment tracking

## ğŸ›  Installation

### Prerequisites

- Python 3.8+
- NVIDIA GPU (recommended for training)
- [CUDA 11.2](https://developer.nvidia.com/cuda-11.2.0-download-archive) and [cuDNN 8.1](https://developer.nvidia.com/cudnn) (for GPU acceleration)

### Setup

```bash
# Clone repository
git clone https://github.com/[your-username]/cloud-masking.git
cd cloud-masking

# Install dependencies
pip install -r requirements.txt

# Download dataset (place in data/raw/)
gdown "https://drive.google.com/uc?id=1-cU2qx7XY_lwCC7PKOnnNRkeyRto80gC"
unzip dataset.zip -d data/raw/
```

## ğŸ— Project Structure

```
cloud-masking/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original dataset (to be downloaded)
â”‚   â”œâ”€â”€ processed/            # Processed data after augmentation/normalization
â”‚   â”œâ”€â”€ train/                # Training split
â”‚   â”‚   â”œâ”€â”€ images/           # Training images
â”‚   â”‚   â””â”€â”€ masks/            # Corresponding masks
â”‚   â””â”€â”€ val/                  # Validation split
â”‚       â”œâ”€â”€ images/           # Validation images
â”‚       â””â”€â”€ masks/            # Corresponding masks
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploratory_data_analysis.ipynb
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚   â”œâ”€â”€ 03_classical_model_experiments.ipynb
â”‚   â””â”€â”€ 04_deep_learning_experiments.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py             # Configuration parameters
â”‚   â”œâ”€â”€ data_loader.py        # Data loading and augmentation
â”‚   â”œâ”€â”€ preprocessing.py      # Image preprocessing utilities
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ unet.py           # UNet implementation
â”‚   â”‚   â”œâ”€â”€ deeplab.py        # DeepLabV3 implementation
â”‚   â”‚   â”œâ”€â”€ random_forest.py  # Classical model
â”‚   â”‚   â””â”€â”€ model_utils.py    # Model utilities
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation metrics
â”‚   â”œâ”€â”€ utils.py              # Helper functions
â”‚   â””â”€â”€ visualization.py      # Visualization utilities
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/               # Saved model weights
â”‚   â”œâ”€â”€ logs/                 # Training logs
â”‚   â””â”€â”€ predictions/          # Prediction outputs
â”œâ”€â”€ run_inference.py          # Inference script for test set
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ Dockerfile                # Docker configuration
â”œâ”€â”€ README.md                 # Project documentation
â””â”€â”€ report/                   # Final report assets
    â”œâ”€â”€ figures/              # Report figures
    â””â”€â”€ Report.pdf # Final PDF report
```

## ğŸ§  Model Performance

| Model         | Dice Coefficient | Inference Time (512px) | Parameters |
| ------------- | ---------------- | ---------------------- | ---------- |
| UNet          | 0.92             | 15ms                   | 7.8M       |
| DeepLabV3+    | 0.91             | 18ms                   | 41.3M      |
| Random Forest | 0.85             | 8ms                    | -          |

_Metrics on validation set (RTX 3080 GPU)_

## ğŸ’» Usage

### Training

```bash
# Train UNet model (default)
python train.py

# Train specific model
python train.py --model deeplab --epochs 50 --batch_size 32
```

### Inference

```bash
# Generate competition submission
python run_inference.py \
    --test_dir path/to/test_images \
    --output_csv submission.csv \
    --model_path outputs/models/unet_best.h5
```

### Docker Support

```bash
# Build and run container
docker build -t cloud-masking .
docker run --gpus all -v $(pwd)/data:/app/data cloud-masking
```

## ğŸ“Š Sample Results

![Prediction Examples](media/image3.png)  
_Left: Input image | Middle: Ground truth | Right: Model prediction_

## ğŸ“š Documentation

- [Full Project Report](report/ST-Project-Report.pdf)
- [API Reference](docs/API.md)
- [Competition Guidelines](docs/COMPETITION.md)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Cairo University Faculty of Engineering for project supervision
- ESA Copernicus Program for sample datasets
- TensorFlow and PyTorch communities for open-source tools
