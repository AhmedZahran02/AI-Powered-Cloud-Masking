{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "sys.path.append(os.path.abspath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, \n",
    "                            recall_score, f1_score, \n",
    "                            confusion_matrix, roc_auc_score,\n",
    "                            classification_report)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from config import PROCESSED_DATA_PATH\n",
    "from utils import load_and_normalize_tiff,load_mask\n",
    "from visualization import plot_image_and_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../outputs\")\n",
    "MODEL_PATH = OUTPUT_PATH / \"models\"\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "CLOUD_CATEGORIES = ['cloud_free', 'partially_clouded', 'fully_clouded']\n",
    "\n",
    "# RF Model parameters\n",
    "TEST_SIZE = 0.2\n",
    "RF_PARAMS = {\n",
    "    'n_estimators': 100,      # number of trees in the forest\n",
    "    'max_depth': 15,          # less depth no overfitting\n",
    "    'min_samples_split': 5,   # regularization\n",
    "    'min_samples_leaf': 5,    # regularization\n",
    "    'max_features': 'sqrt',   \n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,          # Use all cores (cook my CPU)\n",
    "    'verbose': 0,          # debugging\n",
    "    'class_weight': None   # i will handle it in the fit call\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndvi(image):\n",
    "    red = image[:, :, 0].astype('float32')\n",
    "    nir = image[:, :, 3].astype('float32')\n",
    "    ndvi = (nir - red) / (nir + red + 1e-6)\n",
    "    return ndvi\n",
    "\n",
    "def calculate_ndwi(image):\n",
    "    green = image[:, :, 1].astype('float32')\n",
    "    nir = image[:, :, 3].astype('float32')\n",
    "    ndwi = (green - nir) / (green + nir + 1e-6)\n",
    "    return ndwi\n",
    "\n",
    "def calculate_cloud_index(image):\n",
    "    blue = image[:, :, 2].astype('float32')\n",
    "    nir = image[:, :, 3].astype('float32')\n",
    "    ci = (blue - nir) / (blue + nir + 1e-6)\n",
    "    return ci\n",
    "\n",
    "def calculate_ndsi(image):\n",
    "    green = image[:, :, 1].astype('float32')\n",
    "    nir = image[:, :, 3].astype('float32')\n",
    "    return (green - nir) / (green + nir + 1e-6)\n",
    "\n",
    "def extract_features(image):\n",
    "    if image.shape == (512, 512,4):\n",
    "        pass \n",
    "    elif image.shape == (4, 512, 512):\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected image shape: {image.shape}. Expected (4,512,512) or (512,512,4)\")\n",
    "    \n",
    "    features = [\n",
    "        image[:, :, 0].ravel(),     # Red\n",
    "        image[:, :, 1].ravel(),     # Green\n",
    "        image[:, :, 2].ravel(),     # Blue\n",
    "        image[:, :, 3].ravel(),     # IR\n",
    "        calculate_ndvi(image).ravel(),\n",
    "        calculate_ndwi(image).ravel(),\n",
    "        calculate_cloud_index(image).ravel(),\n",
    "        calculate_ndsi(image).ravel()\n",
    "    ]\n",
    "    \n",
    "    return np.column_stack(features) # [r,g,b,ir,ndvi,ndwi,ci,ndsi] for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(base_path):\n",
    "    image_mask_pairs = []\n",
    "    \n",
    "    for category in CLOUD_CATEGORIES:\n",
    "        image_dir = base_path / \"data\" / category\n",
    "        mask_dir = base_path / \"masks\" / category\n",
    "        \n",
    "        if not image_dir.exists() or not mask_dir.exists():\n",
    "            print(f\"Directory is Missing for category {category}\")\n",
    "            continue\n",
    "            \n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.tif')])\n",
    "        mask_files = sorted([f for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
    "        \n",
    "        for img, msk in zip(image_files, mask_files):\n",
    "            image_mask_pairs.append((image_dir / img, mask_dir / msk, category))\n",
    "    \n",
    "    return image_mask_pairs\n",
    "\n",
    "image_mask_pairs = prepare_datasets(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_balanced_data(image_mask_pairs, samples_per_category=100000):\n",
    "    X = []\n",
    "    y = []\n",
    "    categories = []\n",
    "    \n",
    "    # Group by category\n",
    "    category_groups = {}\n",
    "    for img_path, mask_path, category in image_mask_pairs:\n",
    "        if category not in category_groups:\n",
    "            category_groups[category] = []\n",
    "        category_groups[category].append((img_path, mask_path))\n",
    "    \n",
    "    for category, pairs in category_groups.items():\n",
    "        print(f\"\\nProcessing {len(pairs)} {category} images...\")\n",
    "        category_samples = 0\n",
    "        \n",
    "        for img_path, mask_path in tqdm(pairs):\n",
    "            image = load_and_normalize_tiff(img_path)\n",
    "            mask = load_mask(mask_path)\n",
    "            \n",
    "            features = extract_features(image)\n",
    "            labels = mask.ravel()\n",
    "            \n",
    "            # Max samples per category (it will kill my RAM if i load all the pixels)\n",
    "            if samples_per_category:\n",
    "                n_samples = min(samples_per_category - category_samples, features.shape[0])\n",
    "                if n_samples <= 0:\n",
    "                    continue\n",
    "                    \n",
    "                idx = np.random.choice(features.shape[0], n_samples, replace=False)\n",
    "                features = features[idx]\n",
    "                labels = labels[idx]\n",
    "                category_samples += n_samples\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(labels)\n",
    "            categories.extend([category] * len(labels))\n",
    "            \n",
    "            if samples_per_category and category_samples >= samples_per_category:\n",
    "                print(f\"Reached max samples for {category}\")\n",
    "                break\n",
    "    \n",
    "    X = np.vstack(X) if len(X) > 1 else X[0]\n",
    "    y = np.concatenate(y)\n",
    "    categories = np.array(categories)\n",
    "    \n",
    "    return X, y, categories\n",
    "\n",
    "# Balanced sampling\n",
    "X, y, categories = preprocess_balanced_data(image_mask_pairs, samples_per_category=1000000)\n",
    "\n",
    "print(f\"\\nTotal samples: {X.shape[0]}\")\n",
    "print(f\"Feature dimension: {X.shape[1]}\")\n",
    "print(\"Class distribution:\")\n",
    "print(pd.Series(y).value_counts())\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(pd.Series(categories).value_counts())\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(y, bins=20)\n",
    "plt.title('Pixel Class Distribution')\n",
    "plt.xlabel('Class (0=No Cloud, 1=Cloud)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, cat_train, cat_test = train_test_split(\n",
    "    X, y, categories, test_size=TEST_SIZE, \n",
    "    random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape} (categories: {np.unique(cat_train, return_counts=True)})\")\n",
    "print(f\"Test shape: {X_test.shape} (categories: {np.unique(cat_test, return_counts=True)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest classifier...\")\n",
    "\n",
    "# Category weighting\n",
    "category_counts = pd.Series(cat_train).value_counts()\n",
    "category_weights = (1 / (category_counts + 1e-6)) \n",
    "category_weights = category_weights / category_weights.sum()\n",
    "category_weights = category_weights.to_dict()\n",
    "\n",
    "# Pixel weighting\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "# Combine weights\n",
    "sample_weights = np.ones_like(y_train, dtype='float32')\n",
    "for idx, (category, label) in enumerate(zip(cat_train, y_train)):\n",
    "    sample_weights[idx] = category_weights[category] * class_weight_dict[label]\n",
    "\n",
    "print(\"\\nWeight Statistics:\")\n",
    "print(f\"Category weights: {category_weights}\")\n",
    "print(f\"Class weights: {class_weight_dict}\")\n",
    "print(f\"Sample weights - Min: {sample_weights.min():.4f}, Max: {sample_weights.max():.4f}\")\n",
    "print(f\"Sample weights - Mean: {sample_weights.mean():.4f}, Std: {sample_weights.std():.4f}\")\n",
    "\n",
    "rf_model = RandomForestClassifier(**RF_PARAMS)\n",
    "\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, categories=None):\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y, y_pred),\n",
    "        'classification_report': classification_report(y, y_pred, target_names=['No Cloud', 'Cloud'])\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y, y_proba)\n",
    "        except ValueError as e:\n",
    "            print(f\"ROC AUC skipped: {str(e)}\")\n",
    "            metrics['roc_auc'] = None\n",
    "    \n",
    "    # Category metrics\n",
    "    category_results = []\n",
    "    if categories is not None:\n",
    "        for category in np.unique(categories):\n",
    "            mask = categories == category\n",
    "            y_true_sub = y[mask]\n",
    "            \n",
    "            # Skip free cloud categories because it has only single class (0 pixel)\n",
    "            if len(y_true_sub) < 2 or len(np.unique(y_true_sub)) < 2:\n",
    "                continue\n",
    "                \n",
    "            cat_metrics = {\n",
    "                'category': category,\n",
    "                'accuracy': accuracy_score(y_true_sub, y_pred[mask]),\n",
    "                'precision': precision_score(y_true_sub, y_pred[mask], zero_division=0),\n",
    "                'recall': recall_score(y_true_sub, y_pred[mask], zero_division=0),\n",
    "                'f1': f1_score(y_true_sub, y_pred[mask], zero_division=0),\n",
    "                'support': sum(mask)\n",
    "            }\n",
    "            \n",
    "            if y_proba is not None:\n",
    "                try:\n",
    "                    cat_metrics['roc_auc'] = roc_auc_score(y_true_sub, y_proba[mask])\n",
    "                except ValueError:\n",
    "                    cat_metrics['roc_auc'] = None\n",
    "                    \n",
    "            category_results.append(cat_metrics)\n",
    "\n",
    "    metrics.update({\n",
    "        'inference_time': inference_time,\n",
    "        'category_results': category_results\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "test_metrics = evaluate_model(rf_model, X_test, y_test, cat_test)\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(test_metrics['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(test_metrics['classification_report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature importances:\")\n",
    "feature_names = ['Red', 'Green', 'Blue', 'IR', 'NDVI', 'NDWI', 'CloudIndex', 'NDSI']\n",
    "importances = rf_model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances, 'Std': std})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_importance['Feature'], feature_importance['Importance'], yerr=feature_importance['Std'])\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_filename = MODEL_PATH / \"rf_model.pth\"\n",
    "joblib.dump(rf_model, model_filename)\n",
    "\n",
    "# Logging\n",
    "logs = {\n",
    "    'timestamp': time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'model': 'RandomForest',\n",
    "    'training_time_sec': training_time,\n",
    "    'model_size_mb': Path(model_filename).stat().st_size/(1024*1024),\n",
    "    'dataset_stats': {\n",
    "        'total_samples': X.shape[0],\n",
    "        'train_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0],\n",
    "        'class_distribution': dict(zip(*np.unique(y, return_counts=True))),\n",
    "        'category_distribution': dict(zip(*np.unique(categories, return_counts=True))),\n",
    "    },\n",
    "    'weights': {\n",
    "        'category_weights': category_weights,\n",
    "        'class_weights': class_weight_dict,\n",
    "        'sample_weight_stats': {\n",
    "            'min': float(sample_weights.min()),\n",
    "            'max': float(sample_weights.max()),\n",
    "            'mean': float(sample_weights.mean()),\n",
    "            'std': float(sample_weights.std())\n",
    "        }\n",
    "    },\n",
    "    'metrics': {\n",
    "        'test': {k: v for k, v in test_metrics.items() \n",
    "                if k not in ['confusion_matrix', 'classification_report', 'category_results']}\n",
    "    },\n",
    "    'feature_importances': feature_importance.to_dict('records'),\n",
    "    'parameters': RF_PARAMS,\n",
    "    'inference_speed': {\n",
    "        'pixels_per_sec': X_test.shape[0]/test_metrics['inference_time'],\n",
    "        'time_per_pixel': test_metrics['inference_time']/X_test.shape[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {str(k): convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [convert_numpy_types(x) for x in obj]\n",
    "    return obj\n",
    "\n",
    "logs = convert_numpy_types(logs)\n",
    "\n",
    "log_file = OUTPUT_PATH / \"logs\" / \"rf_model_logs.json\"\n",
    "os.makedirs(log_file.parent, exist_ok=True)\n",
    "with open(log_file, 'w') as f:\n",
    "    json.dump(logs, f, indent=2)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"Logs saved to {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "def predict_full_image(model, image_path, output_path=None):\n",
    "    image = load_and_normalize_tiff(image_path)\n",
    "    _, height, width = image.shape\n",
    "    \n",
    "    # Normalization\n",
    "    image = image.astype('float32')\n",
    "    for i in range(image.shape[0]):\n",
    "        if np.max(image[i]) > 0:\n",
    "            image[i] = image[i] / np.max(image[i])\n",
    "    \n",
    "    # Extract features correctly for (4, H, W) format\n",
    "    features = extract_features(image)\n",
    "    \n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(features)\n",
    "        cloud_proba = proba[:, 1]\n",
    "        proba_map = cloud_proba.reshape((height, width))\n",
    "    else:\n",
    "        mask_flat = model.predict(features)\n",
    "        proba_map = mask_flat.reshape((height, width)).astype('float32')\n",
    "    \n",
    "    high_threshold = 0.85 \n",
    "    low_threshold = 0.8  \n",
    "    \n",
    "    strong_clouds = proba_map > high_threshold\n",
    "    weak_clouds = (proba_map > low_threshold) & (proba_map <= high_threshold)\n",
    "    \n",
    "    from scipy import ndimage\n",
    "    \n",
    "    # Small dilation on strong clouds\n",
    "    struct = ndimage.generate_binary_structure(2, 2)\n",
    "    dilated_strong = ndimage.binary_dilation(strong_clouds, structure=struct, iterations=1)\n",
    "    \n",
    "    # Connect weak clouds that are near strong clouds\n",
    "    final_mask = dilated_strong.copy()\n",
    "    \n",
    "    # Add weak clouds that are connected to strong clouds\n",
    "    labeled_weak, num_labels = ndimage.label(weak_clouds)\n",
    "    if num_labels > 0:\n",
    "        for label in range(1, num_labels + 1):\n",
    "            if np.any(dilated_strong & (labeled_weak == label)):\n",
    "                final_mask |= (labeled_weak == label)\n",
    "    \n",
    "    # Fill small holes in clouds\n",
    "    final_mask = ndimage.binary_fill_holes(final_mask)\n",
    "    \n",
    "    # Remove small isolated cloud patches (smaller than 20 pixels)\n",
    "    labeled_final, num_labels = ndimage.label(final_mask)\n",
    "    component_sizes = np.bincount(labeled_final.ravel())\n",
    "    small_components = component_sizes < 20\n",
    "    small_components[0] = False\n",
    "    remove_pixels = small_components[labeled_final]\n",
    "    final_mask[remove_pixels] = False\n",
    "    \n",
    "    binary_mask = final_mask.astype(np.uint8)\n",
    "    \n",
    "    plot_image_and_mask(image, binary_mask, title=\"Predicted Mask\", save_path=output_path)\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "# 289430\n",
    "# 327408 Failed\n",
    "# 130726\n",
    "# 140804\n",
    "# 262934\n",
    "predict_full_image(rf_model, \"test/data/289430.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
