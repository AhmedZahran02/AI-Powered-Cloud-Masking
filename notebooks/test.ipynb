{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0e9ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08188aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud Detection Pipeline\n",
      "=======================\n",
      "Finding image pairs...\n",
      "Found 10574 image pairs\n",
      "\n",
      "Splitting datasets...\n",
      "Train: 7401, Validation: 2115, Test: 1058\n",
      "\n",
      "Calculating class weights...\n",
      "Class weights: {0: 1.2817360952698484, 1: 0.8198010191837211}\n",
      "\n",
      "Running cross-validation...\n",
      "Starting 3-fold cross-validation...\n",
      "\n",
      "Fold 1/3\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8886964fe6aa44b69a1c58b312d9072a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train data:   0%|          | 0/4934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cloud_detection.py - Memory Efficient Cloud Detection with Classical ML\n",
    "Uses stratified sampling, optimized feature engineering, and ensemble techniques\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import uniform_filter, minimum_filter, maximum_filter\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.util import view_as_windows\n",
    "from skimage.exposure import equalize_hist\n",
    "from rasterio.errors import NotGeoreferencedWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    PROCESSED_DATA = Path(\"../data/processed\")\n",
    "    OUTPUT_PATH = Path(\"../outputs\")\n",
    "    MODEL_PATH = OUTPUT_PATH / \"models\"\n",
    "    FEATURES_PATH = OUTPUT_PATH / \"features\"\n",
    "    \n",
    "    # Create directories\n",
    "    for path in [MODEL_PATH, FEATURES_PATH, OUTPUT_PATH / \"logs\", OUTPUT_PATH / \"predictions\"]:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "    \n",
    "    # Data processing\n",
    "    TILE_SIZE = 128  # Process images in smaller tiles to save memory\n",
    "    MAX_SAMPLES_PER_IMAGE = 10000  # Cap samples from each image\n",
    "    SAMPLES_PER_CATEGORY = 100000  # Balance samples across categories\n",
    "    PRECOMPUTE_FEATURES = True  # Store features on disk\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 5000  # Pixels per batch\n",
    "    N_EPOCHS = 15\n",
    "    VAL_SPLIT = 0.2\n",
    "    TEST_SPLIT = 0.1\n",
    "    CROSS_VAL_FOLDS = 3  # Number of cross-validation folds\n",
    "    \n",
    "    # Early stopping\n",
    "    EARLY_STOP_PATIENCE = 4\n",
    "    EARLY_STOP_DELTA = 0.001\n",
    "    \n",
    "    # Feature engineering\n",
    "    SPECTRAL_INDICES = True\n",
    "    SPATIAL_WINDOW_SIZES = [3, 5, 7]  # Multi-scale spatial features\n",
    "    TEXTURE_WINDOW = 7\n",
    "    FEATURE_SELECTION = True  # Use feature selection\n",
    "    MAX_FEATURES = 30  # Max number of features to keep\n",
    "    \n",
    "    # Model parameters\n",
    "    SGD_PARAMS = {\n",
    "        'loss': 'modified_huber',\n",
    "        'penalty': 'elasticnet',\n",
    "        'alpha': 0.0005,\n",
    "        'l1_ratio': 0.15,\n",
    "        'learning_rate': 'adaptive',\n",
    "        'eta0': 0.02,\n",
    "        'max_iter': 1,\n",
    "        'class_weight': 'balanced',\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    RF_PARAMS = {\n",
    "        'n_estimators': 50,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 4,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'class_weight': 'balanced',\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    SVM_PARAMS = {\n",
    "        'penalty': 'l2',\n",
    "        'loss': 'squared_hinge',\n",
    "        'dual': False,\n",
    "        'C': 0.1,\n",
    "        'class_weight': 'balanced',\n",
    "        'max_iter': 1000\n",
    "    }\n",
    "\n",
    "    # Post-processing\n",
    "    POST_PROCESS = True\n",
    "    MORPHOLOGY_SIZE = 3\n",
    "    PROBABILITY_THRESHOLD = 0.4  # Lower than 0.5 to catch more clouds\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================================\n",
    "# Utilities\n",
    "# ============================================================================\n",
    "def load_and_normalize_tiff(path):\n",
    "    \"\"\"Load TIFF image and normalize values to [0, 1]\"\"\"\n",
    "    try:\n",
    "        import rasterio\n",
    "        with rasterio.open(path) as src:\n",
    "            image = src.read()\n",
    "            \n",
    "        # Handle different band counts\n",
    "        if image.shape[0] != 4:\n",
    "            raise ValueError(f\"Expected 4 bands, got {image.shape[0]}\")\n",
    "            \n",
    "        # Scale each band independently to [0, 1]\n",
    "        for i in range(image.shape[0]):\n",
    "            band = image[i]\n",
    "            min_val, max_val = np.percentile(band, (1, 99))  # Robust scaling\n",
    "            band = np.clip((band - min_val) / (max_val - min_val + 1e-8), 0, 1)\n",
    "            image[i] = band\n",
    "            \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_mask(path):\n",
    "    \"\"\"Load binary mask\"\"\"\n",
    "    try:\n",
    "        import rasterio\n",
    "        with rasterio.open(path) as src:\n",
    "            mask = src.read(1).astype(np.uint8)\n",
    "        return mask\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mask {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_tiles(image, tile_size=128, overlap=0):\n",
    "    \"\"\"Split image into tiles for memory-efficient processing\"\"\"\n",
    "    if len(image.shape) == 3:  # Multi-band image\n",
    "        h, w = image.shape[1], image.shape[2]\n",
    "        tiles = []\n",
    "        coords = []\n",
    "        \n",
    "        for y in range(0, h - overlap, tile_size - overlap):\n",
    "            if y + tile_size > h:\n",
    "                y = h - tile_size\n",
    "                \n",
    "            for x in range(0, w - overlap, tile_size - overlap):\n",
    "                if x + tile_size > w:\n",
    "                    x = w - tile_size\n",
    "                    \n",
    "                tile = image[:, y:y+tile_size, x:x+tile_size]\n",
    "                tiles.append(tile)\n",
    "                coords.append((y, x))\n",
    "                \n",
    "        return tiles, coords\n",
    "    else:  # Single band (mask)\n",
    "        h, w = image.shape\n",
    "        tiles = []\n",
    "        coords = []\n",
    "        \n",
    "        for y in range(0, h - overlap, tile_size - overlap):\n",
    "            if y + tile_size > h:\n",
    "                y = h - tile_size\n",
    "                \n",
    "            for x in range(0, w - overlap, tile_size - overlap):\n",
    "                if x + tile_size > w:\n",
    "                    x = w - tile_size\n",
    "                    \n",
    "                tile = image[y:y+tile_size, x:x+tile_size]\n",
    "                tiles.append(tile)\n",
    "                coords.append((y, x))\n",
    "                \n",
    "        return tiles, coords\n",
    "\n",
    "# ============================================================================\n",
    "# Feature Engineering\n",
    "# ============================================================================\n",
    "def calculate_spectral_indices(image):\n",
    "    \"\"\"Calculate multiple spectral indices\"\"\"\n",
    "    # Ensure float32 dtype and handle NaN values\n",
    "    bands = []\n",
    "    for i in range(image.shape[0]):\n",
    "        band = image[i].astype('float32')\n",
    "        band = np.nan_to_num(band, nan=0.0)\n",
    "        bands.append(band)\n",
    "    \n",
    "    # Unpack bands\n",
    "    if len(bands) == 4:\n",
    "        blue, green, red, nir = bands\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 4 bands, got {len(bands)}\")\n",
    "    \n",
    "    indices = {}\n",
    "    eps = 1e-6  # Prevent division by zero\n",
    "    \n",
    "    # Standard indices\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        # NDVI (Normalized Difference Vegetation Index)\n",
    "        indices['ndvi'] = np.nan_to_num((nir - red) / (nir + red + eps), nan=0.0)\n",
    "        \n",
    "        # NDWI (Normalized Difference Water Index)\n",
    "        indices['ndwi'] = np.nan_to_num((green - nir) / (green + nir + eps), nan=0.0)\n",
    "        \n",
    "        # Enhanced Vegetation Index (EVI)\n",
    "        indices['evi'] = np.nan_to_num(2.5 * (nir - red) / (nir + 6 * red - 7.5 * blue + 1 + eps), nan=0.0)\n",
    "        \n",
    "        # Visible Atmospherically Resistant Index (VARI)\n",
    "        indices['vari'] = np.nan_to_num((green - red) / (green + red - blue + eps), nan=0.0)\n",
    "        \n",
    "        # Simple Ratio (SR)\n",
    "        indices['sr'] = np.nan_to_num(nir / (red + eps), nan=0.0)\n",
    "        \n",
    "        # Shadow index\n",
    "        indices['si'] = np.nan_to_num((1 - blue) * (1 - green) * (1 - red), nan=0.0)\n",
    "        \n",
    "        # Normalized Difference Built-up Index (NDBI)\n",
    "        if nir is not None:\n",
    "            indices['ndbi'] = np.nan_to_num((nir - green) / (nir + green + eps), nan=0.0)\n",
    "    \n",
    "    # Clip extreme values\n",
    "    for key in indices:\n",
    "        indices[key] = np.clip(indices[key], -1, 1)\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def calculate_spatial_features(band, window_size):\n",
    "    \"\"\"Calculate spatial features with uniform filtering\"\"\"\n",
    "    # Mean and variance\n",
    "    mean = uniform_filter(band, size=window_size)\n",
    "    mean_sq = uniform_filter(band**2, size=window_size)\n",
    "    variance = np.maximum(mean_sq - mean**2, 0.0)  # Ensure non-negative\n",
    "    std = np.sqrt(variance)\n",
    "    \n",
    "    # Min, max, range\n",
    "    min_val = minimum_filter(band, size=window_size)\n",
    "    max_val = maximum_filter(band, size=window_size)\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    # Edge detection (approximate gradient magnitude)\n",
    "    sobel_h = ndimage.sobel(band, axis=0)\n",
    "    sobel_v = ndimage.sobel(band, axis=1)\n",
    "    edge = np.sqrt(sobel_h**2 + sobel_v**2)\n",
    "    \n",
    "    return np.stack([mean, std, min_val, max_val, range_val, edge], axis=-1)\n",
    "\n",
    "def calculate_texture_features(band, window_size):\n",
    "    \"\"\"Calculate GLCM-like texture features\"\"\"\n",
    "    # Simple texture descriptors without computing full GLCM\n",
    "    features = []\n",
    "    \n",
    "    # Local entropy (approximation)\n",
    "    entropy = uniform_filter(band * np.log(band + 1e-10), size=window_size)\n",
    "    features.append(entropy)\n",
    "    \n",
    "    # Contrast-like (local variance)\n",
    "    mean = uniform_filter(band, size=window_size)\n",
    "    variance = uniform_filter((band - mean)**2, size=window_size)\n",
    "    features.append(variance)\n",
    "    \n",
    "    # Homogeneity-like (inverse of local range)\n",
    "    min_val = minimum_filter(band, size=window_size)\n",
    "    max_val = maximum_filter(band, size=window_size)\n",
    "    range_val = max_val - min_val + 1e-6\n",
    "    homogeneity = 1.0 / range_val\n",
    "    features.append(homogeneity)\n",
    "    \n",
    "    return np.stack(features, axis=-1)\n",
    "\n",
    "def extract_features(image, indices=None):\n",
    "    \"\"\"Extract comprehensive feature set from image\"\"\"\n",
    "    if image.shape[0] != 4:\n",
    "        raise ValueError(f\"Expected 4 bands, got {image.shape[0]}\")\n",
    "    \n",
    "    # Transpose to (H, W, C) for easier processing\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    h, w, c = image.shape\n",
    "    n_pixels = h * w\n",
    "    \n",
    "    # Calculate spectral indices if not provided\n",
    "    if indices is None and config.SPECTRAL_INDICES:\n",
    "        image_transposed = np.transpose(image, (2, 0, 1))  # Back to (C, H, W)\n",
    "        indices = calculate_spectral_indices(image_transposed)\n",
    "    \n",
    "    # Determine feature count\n",
    "    n_spectral_indices = len(indices) if indices else 0\n",
    "    n_spatial_per_band = 6  # mean, std, min, max, range, edge\n",
    "    n_texture_per_band = 3  # entropy, variance, homogeneity\n",
    "    n_scales = len(config.SPATIAL_WINDOW_SIZES)\n",
    "    \n",
    "    # Calculate total feature count\n",
    "    total_features = (\n",
    "        c +  # Raw bands\n",
    "        n_spectral_indices +  # Spectral indices\n",
    "        c * n_spatial_per_band * n_scales +  # Multi-scale spatial features\n",
    "        c * n_texture_per_band  # Texture features\n",
    "    )\n",
    "    \n",
    "    # Initialize feature array\n",
    "    features = np.empty((n_pixels, total_features), dtype='float32')\n",
    "    \n",
    "    # 1. Raw bands\n",
    "    features[:, 0:c] = image.reshape(n_pixels, c)\n",
    "    \n",
    "    # 2. Spectral indices\n",
    "    if indices:\n",
    "        col_idx = c\n",
    "        for idx_name, idx_values in indices.items():\n",
    "            features[:, col_idx] = idx_values.ravel()\n",
    "            col_idx += 1\n",
    "    else:\n",
    "        col_idx = c\n",
    "    \n",
    "    # 3. Multi-scale spatial features\n",
    "    for band_idx in range(c):\n",
    "        band = image[:, :, band_idx]\n",
    "        \n",
    "        for scale_idx, window_size in enumerate(config.SPATIAL_WINDOW_SIZES):\n",
    "            spatial = calculate_spatial_features(band, window_size)\n",
    "            start_col = col_idx + band_idx * n_spatial_per_band * n_scales + scale_idx * n_spatial_per_band\n",
    "            end_col = start_col + n_spatial_per_band\n",
    "            features[:, start_col:end_col] = spatial.reshape(n_pixels, n_spatial_per_band)\n",
    "    \n",
    "    # Update column index\n",
    "    col_idx = c + n_spectral_indices + c * n_spatial_per_band * n_scales\n",
    "    \n",
    "    # 4. Texture features\n",
    "    for band_idx in range(c):\n",
    "        band = image[:, :, band_idx]\n",
    "        texture = calculate_texture_features(band, config.TEXTURE_WINDOW)\n",
    "        start_col = col_idx + band_idx * n_texture_per_band\n",
    "        end_col = start_col + n_texture_per_band\n",
    "        features[:, start_col:end_col] = texture.reshape(n_pixels, n_texture_per_band)\n",
    "    \n",
    "    # Clean up any remaining NaNs or infs\n",
    "    features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# ============================================================================\n",
    "# Data Pipeline\n",
    "# ============================================================================\n",
    "class CloudDataset:\n",
    "    def __init__(self, image_pairs, mode='train', feature_cache_dir=None):\n",
    "        self.image_pairs = image_pairs\n",
    "        self.mode = mode\n",
    "        self.feature_cache_dir = feature_cache_dir\n",
    "        if feature_cache_dir:\n",
    "            os.makedirs(feature_cache_dir, exist_ok=True)\n",
    "        self.class_weights = None\n",
    "    \n",
    "    def _get_cache_path(self, img_path):\n",
    "        \"\"\"Get path for cached features\"\"\"\n",
    "        if self.feature_cache_dir:\n",
    "            img_name = img_path.stem\n",
    "            return Path(self.feature_cache_dir) / f\"{img_name}_features.npz\"\n",
    "        return None\n",
    "    \n",
    "    def _stratified_sampling(self, mask, n_samples, random_state=None):\n",
    "        \"\"\"Balanced sampling preserving class ratio with optional fixed random state\"\"\"\n",
    "        # Input validation\n",
    "        if not isinstance(mask, np.ndarray):\n",
    "            raise ValueError(\"Mask must be a numpy array\")\n",
    "        if n_samples <= 0:\n",
    "            return np.array([], dtype=np.int32)\n",
    "        \n",
    "        rng = np.random.RandomState(random_state) if random_state is not None else np.random\n",
    "        \n",
    "        flat_mask = mask.ravel()\n",
    "        cloud_idx = np.where(flat_mask == 1)[0]\n",
    "        clear_idx = np.where(flat_mask == 0)[0]\n",
    "        \n",
    "        # Calculate sample counts with protection against edge cases\n",
    "        n_cloud = min(len(cloud_idx), max(int(n_samples * 0.5), 100))\n",
    "        n_clear = min(len(clear_idx), max(n_samples - n_cloud, 0))\n",
    "        \n",
    "        # Adjust sample counts if one class is under-represented\n",
    "        if n_clear < (n_samples - n_cloud):\n",
    "            n_cloud = min(len(cloud_idx), max(n_samples - n_clear, 0))\n",
    "        \n",
    "        # Perform sampling with type safety\n",
    "        sampled_cloud = np.array([], dtype=np.int32)\n",
    "        sampled_clear = np.array([], dtype=np.int32)\n",
    "        \n",
    "        if n_cloud > 0 and len(cloud_idx) > 0:\n",
    "            sampled_cloud = rng.choice(cloud_idx, size=n_cloud, replace=False)\n",
    "            sampled_cloud = sampled_cloud.astype(np.int32)\n",
    "        \n",
    "        if n_clear > 0 and len(clear_idx) > 0:\n",
    "            sampled_clear = rng.choice(clear_idx, size=n_clear, replace=False)\n",
    "            sampled_clear = sampled_clear.astype(np.int32)\n",
    "        \n",
    "        # Combine results with type consistency\n",
    "        combined = np.concatenate([sampled_cloud, sampled_clear])\n",
    "        return combined.astype(np.int32) if len(combined) > 0 else np.array([], dtype=np.int32)\n",
    "    \n",
    "    def process_image_pair(self, img_path, mask_path, category, random_state=None):\n",
    "        \"\"\"Process a single image-mask pair, with caching\"\"\"\n",
    "        cache_path = self._get_cache_path(img_path)\n",
    "\n",
    "        # Try to load from cache first\n",
    "        if cache_path and cache_path.exists() and config.PRECOMPUTE_FEATURES:\n",
    "            try:\n",
    "                cached = np.load(cache_path)\n",
    "                features = cached['features']\n",
    "                labels = cached['labels']\n",
    "                return features, labels\n",
    "            except Exception as e:\n",
    "                print(f\"Cache read error for {img_path.name}: {e}\")\n",
    "\n",
    "        # Load data\n",
    "        image = load_and_normalize_tiff(img_path)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "\n",
    "        mask = load_mask(mask_path)\n",
    "        if mask is None:\n",
    "            return None, None\n",
    "\n",
    "        # Split into tiles for memory efficiency\n",
    "        img_tiles, img_coords = generate_tiles(image, config.TILE_SIZE)\n",
    "        mask_tiles, _ = generate_tiles(mask, config.TILE_SIZE)\n",
    "\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Process each tile\n",
    "        for img_tile, mask_tile in zip(img_tiles, mask_tiles):\n",
    "            # Calculate indices once per tile\n",
    "            indices = calculate_spectral_indices(img_tile)\n",
    "\n",
    "            # Extract features\n",
    "            features = extract_features(img_tile, indices)\n",
    "\n",
    "            # Get labels\n",
    "            labels = mask_tile.ravel()\n",
    "\n",
    "            # Sampling for balanced classes\n",
    "            if self.mode == 'train':\n",
    "                max_samples = min(config.MAX_SAMPLES_PER_IMAGE // len(img_tiles), len(labels))\n",
    "                if max_samples > 0:  # Only sample if we have samples to take\n",
    "                    idx = self._stratified_sampling(mask_tile, max_samples, random_state)\n",
    "                    if len(idx) > 0:  # Check we got valid indices\n",
    "                        idx = np.asarray(idx, dtype=np.int32)  # Ensure integer type\n",
    "                        features = features[idx]\n",
    "                        labels = labels[idx]\n",
    "\n",
    "            if len(features) > 0 and len(labels) > 0:  # Only add if we have data\n",
    "                all_features.append(features)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        # Combine results from all tiles\n",
    "        if all_features:\n",
    "            features = np.vstack(all_features)\n",
    "            labels = np.hstack(all_labels)\n",
    "\n",
    "            # Save to cache\n",
    "            if cache_path and config.PRECOMPUTE_FEATURES:\n",
    "                try:\n",
    "                    np.savez_compressed(cache_path, features=features, labels=labels)\n",
    "                except Exception as e:\n",
    "                    print(f\"Cache write error for {img_path.name}: {e}\")\n",
    "\n",
    "            return features, labels\n",
    "        return None, None\n",
    "    \n",
    "    def batch_generator(self, random_state=None):\n",
    "        \"\"\"Generate batches of features and labels\"\"\"\n",
    "        # Process all image pairs in parallel\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Create wrapper function for thread_map\n",
    "        def process_wrapper(args):\n",
    "            img_path, mask_path, category = args\n",
    "            return self.process_image_pair(img_path, mask_path, category, random_state=random_state)\n",
    "\n",
    "        # Use thread_map for parallel processing with progress bar\n",
    "        max_workers = min(os.cpu_count(), 16)\n",
    "        results = thread_map(process_wrapper,\n",
    "                            self.image_pairs,\n",
    "                            max_workers=max_workers,\n",
    "                            desc=f\"Processing {self.mode} data\")\n",
    "\n",
    "        # Collect results\n",
    "        for features, labels in results:\n",
    "            if features is not None and labels is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(labels)\n",
    "\n",
    "        if not all_features:\n",
    "            print(f\"No valid features found in {self.mode} dataset!\")\n",
    "            return\n",
    "\n",
    "        # Combine all data\n",
    "        X = np.vstack(all_features)\n",
    "        y = np.hstack(all_labels)\n",
    "\n",
    "        # For validation/test, we might want to use all data\n",
    "        if self.mode != 'train':\n",
    "            # Still cap maximum size for very large datasets\n",
    "            if len(y) > config.MAX_SAMPLES_PER_IMAGE * 10:\n",
    "                idx = np.random.choice(len(y), config.MAX_SAMPLES_PER_IMAGE * 10, replace=False)\n",
    "                X = X[idx]\n",
    "                y = y[idx]\n",
    "\n",
    "            # Return all validation/test data at once\n",
    "            yield X, y\n",
    "            return\n",
    "\n",
    "        # For training, shuffle and batch\n",
    "        indices = np.arange(len(y))\n",
    "        rng = np.random.RandomState(random_state) if random_state is not None else np.random\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        # Generate batches\n",
    "        start_idx = 0\n",
    "        while start_idx < len(indices):\n",
    "            batch_indices = indices[start_idx:start_idx + config.BATCH_SIZE]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            yield X_batch, y_batch\n",
    "            start_idx += config.BATCH_SIZE\n",
    "    \n",
    "    def calculate_class_weights(self):\n",
    "        \"\"\"Compute global class weights\"\"\"\n",
    "        class_counts = {0: 0, 1: 0}\n",
    "        \n",
    "        for _, mask_path, _ in self.image_pairs:\n",
    "            mask = load_mask(mask_path)\n",
    "            if mask is not None:\n",
    "                unique, counts = np.unique(mask, return_counts=True)\n",
    "                for cls, cnt in zip(unique, counts):\n",
    "                    if cls in class_counts:\n",
    "                        class_counts[cls] += cnt\n",
    "        \n",
    "        total = sum(class_counts.values())\n",
    "        if total > 0 and class_counts[0] > 0 and class_counts[1] > 0:\n",
    "            self.class_weights = {\n",
    "                0: total / (2 * class_counts[0]),\n",
    "                1: total / (2 * class_counts[1])\n",
    "            }\n",
    "        else:\n",
    "            print(\"Warning: Unable to calculate class weights, using default\")\n",
    "            self.class_weights = {0: 1.0, 1: 1.0}\n",
    "        \n",
    "        return self.class_weights\n",
    "\n",
    "# ============================================================================\n",
    "# Model Pipeline\n",
    "# ============================================================================\n",
    "class CloudDetectionModel:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.feature_selector = None\n",
    "        self.scaler = None\n",
    "        self.selected_features = None\n",
    "        self.feature_importances = None\n",
    "    \n",
    "    def _create_ensemble(self, class_weights=None):\n",
    "        \"\"\"Create a voting ensemble of classifiers\"\"\"\n",
    "        # SGD Classifier\n",
    "        sgd = SGDClassifier(**{\n",
    "            **config.SGD_PARAMS,\n",
    "            'class_weight': class_weights\n",
    "        })\n",
    "        \n",
    "        # Random Forest\n",
    "        rf = RandomForestClassifier(**{\n",
    "            **config.RF_PARAMS,\n",
    "            'class_weight': class_weights\n",
    "        })\n",
    "        \n",
    "        # Linear SVM (calibrated for probability outputs)\n",
    "        svm = CalibratedClassifierCV(\n",
    "            LinearSVC(**{\n",
    "                **config.SVM_PARAMS, \n",
    "                'class_weight': class_weights\n",
    "            }),\n",
    "            cv=3\n",
    "        )\n",
    "        \n",
    "        # Create voting ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('sgd', sgd),\n",
    "                ('rf', rf),\n",
    "                ('svm', svm)\n",
    "            ],\n",
    "            voting='soft',  # Use probability estimates\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return ensemble\n",
    "    \n",
    "    def select_features(self, X, y):\n",
    "        \"\"\"Perform feature selection\"\"\"\n",
    "        print(\"Performing feature selection...\")\n",
    "        \n",
    "        # Initialize and fit a RandomForest for feature importance\n",
    "        rf_selector = RandomForestClassifier(\n",
    "            n_estimators=50,\n",
    "            max_depth=10,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        rf_selector.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        importances = rf_selector.feature_importances_\n",
    "        self.feature_importances = importances\n",
    "        \n",
    "        # Select top features\n",
    "        selector = SelectFromModel(\n",
    "            rf_selector,\n",
    "            threshold=-np.inf,  # Keep all features initially\n",
    "            prefit=True,\n",
    "            max_features=config.MAX_FEATURES\n",
    "        )\n",
    "        \n",
    "        X_selected = selector.transform(X)\n",
    "        self.feature_selector = selector\n",
    "        \n",
    "        # Record selected feature indices\n",
    "        self.selected_features = selector.get_support(indices=True)\n",
    "        \n",
    "        print(f\"Selected {len(self.selected_features)} features out of {X.shape[1]}\")\n",
    "        return X_selected\n",
    "    \n",
    "    def fit(self, train_gen_func, val_gen_func=None, class_weights=None):\n",
    "        \"\"\"Train the model with early stopping\"\"\"\n",
    "        print(\"Starting model training...\")\n",
    "        \n",
    "        # Get initial batch of data for feature selection and scaling\n",
    "        for X_train, y_train in train_gen_func():\n",
    "            break\n",
    "        \n",
    "        # Initial preprocessing\n",
    "        print(f\"Initial data shape: {X_train.shape}\")\n",
    "        \n",
    "        # Feature scaling\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Feature selection\n",
    "        if config.FEATURE_SELECTION:\n",
    "            X_train = self.select_features(X_train, y_train)\n",
    "        \n",
    "        # Create model\n",
    "        print(\"Creating model...\")\n",
    "        self.model = self._create_ensemble(class_weights)\n",
    "        \n",
    "        # Training loop\n",
    "        best_f1 = 0\n",
    "        no_improvement_count = 0\n",
    "        val_metrics_history = []\n",
    "        \n",
    "        for epoch in range(config.N_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{config.N_EPOCHS}\")\n",
    "            \n",
    "            # Training phase\n",
    "            batch_count = 0\n",
    "            for X_batch, y_batch in tqdm(train_gen_func(), desc=\"Training\"):\n",
    "                # Apply preprocessing\n",
    "                X_batch = self.scaler.transform(X_batch)\n",
    "                if config.FEATURE_SELECTION:\n",
    "                    X_batch = self.feature_selector.transform(X_batch)\n",
    "                \n",
    "                # Partial fit for incremental learning\n",
    "                if hasattr(self.model, 'partial_fit'):\n",
    "                    self.model.partial_fit(X_batch, y_batch, classes=[0, 1])\n",
    "                else:  # For non-incremental models, accumulate data and fit once\n",
    "                    if batch_count == 0:\n",
    "                        X_accumulated = X_batch\n",
    "                        y_accumulated = y_batch\n",
    "                    else:\n",
    "                        X_accumulated = np.vstack([X_accumulated, X_batch])\n",
    "                        y_accumulated = np.hstack([y_accumulated, y_batch])\n",
    "                \n",
    "                batch_count += 1\n",
    "            \n",
    "            # For non-incremental models, fit on accumulated data\n",
    "            if not hasattr(self.model, 'partial_fit') and batch_count > 0:\n",
    "                print(f\"Fitting model on {len(y_accumulated)} samples...\")\n",
    "                self.model.fit(X_accumulated, y_accumulated)\n",
    "            \n",
    "            print(f\"Trained on {batch_count} batches\")\n",
    "            \n",
    "            # Validation phase\n",
    "            if val_gen_func:\n",
    "                val_preds, val_true, val_proba = [], [], []\n",
    "                \n",
    "                for X_val, y_val in val_gen_func():\n",
    "                    # Apply preprocessing\n",
    "                    X_val = self.scaler.transform(X_val)\n",
    "                    if config.FEATURE_SELECTION:\n",
    "                        X_val = self.feature_selector.transform(X_val)\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    val_preds.extend(self.model.predict(X_val))\n",
    "                    val_true.extend(y_val)\n",
    "                    \n",
    "                    # Get probabilities if available\n",
    "                    if hasattr(self.model, 'predict_proba'):\n",
    "                        val_proba.extend(self.model.predict_proba(X_val)[:, 1])\n",
    "                \n",
    "                if val_true:\n",
    "                    # Calculate metrics\n",
    "                    metrics = calculate_metrics(val_true, val_preds, val_proba if val_proba else None)\n",
    "                    val_metrics_history.append(metrics)\n",
    "                    \n",
    "                    current_f1 = metrics['f1']\n",
    "                    print(f\"Val F1: {current_f1:.4f} | Best: {best_f1:.4f}\")\n",
    "                    \n",
    "                    # Early stopping check\n",
    "                    if current_f1 > best_f1 + config.EARLY_STOP_DELTA:\n",
    "                        best_f1 = current_f1\n",
    "                        no_improvement_count = 0\n",
    "                        # Save best model\n",
    "                        self.save(config.MODEL_PATH / \"best_model.joblib\")\n",
    "                        print(\"↑ New best model saved ↑\")\n",
    "                    else:\n",
    "                        no_improvement_count += 1\n",
    "                        print(f\"No improvement ({no_improvement_count}/{config.EARLY_STOP_PATIENCE})\")\n",
    "                        \n",
    "                        if no_improvement_count >= config.EARLY_STOP_PATIENCE:\n",
    "                            print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "                            break\n",
    "        \n",
    "        # Save final model\n",
    "        self.save(config.MODEL_PATH / \"final_model.joblib\")\n",
    "        return val_metrics_history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make binary predictions\"\"\"\n",
    "        # Preprocess\n",
    "        X = self.scaler.transform(X)\n",
    "        if config.FEATURE_SELECTION and self.feature_selector is not None:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        \n",
    "        # Predict\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probabilities\"\"\"\n",
    "        # Preprocess\n",
    "        X = self.scaler.transform(X)\n",
    "        if config.FEATURE_SELECTION and self.feature_selector is not None:\n",
    "            X = self.feature_selector.transform(X)\n",
    "        \n",
    "        # Predict probabilities\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            return self.model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            # Fall back to binary predictions\n",
    "            return self.predict(X).astype(float)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model and preprocessing components\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'selected_features': self.selected_features,\n",
    "            'feature_importances': self.feature_importances\n",
    "        }\n",
    "        joblib.dump(model_data, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\"Load model from file\"\"\"\n",
    "        model_data = joblib.load(path)\n",
    "        \n",
    "        model = cls()\n",
    "        model.model = model_data['model']\n",
    "        model.scaler = model_data['scaler']\n",
    "        model.feature_selector = model_data['feature_selector']\n",
    "        model.selected_features = model_data['selected_features']\n",
    "        model.feature_importances = model_data['feature_importances']\n",
    "        return model\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation and Metrics\n",
    "# ============================================================================\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),\n",
    "        'classification_report': classification_report(y_true, y_pred, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None and len(np.unique(y_true)) > 1:\n",
    "        try:\n",
    "            metrics['roc_auc'] = roc_auc_score(y_true, y_proba)\n",
    "        except Exception:\n",
    "            metrics['roc_auc'] = 0.5  # Default value if calculation fails\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def save_metrics(metrics, name, output_dir):\n",
    "    \"\"\"Save metrics to JSON file\"\"\"\n",
    "    log_dir = output_dir / \"logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    metrics_path = log_dir / f\"{name}_metrics.json\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    print(f\"Saved {name} metrics to {metrics_path}\")\n",
    "\n",
    "def plot_feature_importance(model, output_path=None):\n",
    "    \"\"\"Plot feature importance if available\"\"\"\n",
    "    if model.feature_importances is not None and model.selected_features is not None:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Only plot for selected features\n",
    "        indices = np.argsort(model.feature_importances)[-20:]  # Top 20 features\n",
    "        plt.barh(range(len(indices)), model.feature_importances[indices])\n",
    "        plt.yticks(range(len(indices)), [f\"Feature {i}\" for i in indices])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('Top 20 Important Features')\n",
    "        \n",
    "        if output_path:\n",
    "            plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Feature importance plot saved to {output_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Cross-Validation\n",
    "# ============================================================================\n",
    "def cross_validate(image_pairs, n_folds=5, random_state=42):\n",
    "    \"\"\"Perform cross-validation\"\"\"\n",
    "    print(f\"Starting {n_folds}-fold cross-validation...\")\n",
    "    \n",
    "    # Prepare folds\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Extract categories for stratification\n",
    "    categories = [category for _, _, category in image_pairs]\n",
    "    category_ids = {cat: i for i, cat in enumerate(set(categories))}\n",
    "    stratify_values = [category_ids[cat] for cat in categories]\n",
    "    \n",
    "    cv_metrics = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(image_pairs, stratify_values)):\n",
    "        print(f\"\\nFold {fold+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data\n",
    "        train_pairs = [image_pairs[i] for i in train_idx]\n",
    "        val_pairs = [image_pairs[i] for i in val_idx]\n",
    "        \n",
    "        # Create datasets\n",
    "        train_ds = CloudDataset(\n",
    "            train_pairs, \n",
    "            mode='train',\n",
    "            feature_cache_dir=config.FEATURES_PATH / f\"fold_{fold+1}_train\"\n",
    "        )\n",
    "        val_ds = CloudDataset(\n",
    "            val_pairs, \n",
    "            mode='val',\n",
    "            feature_cache_dir=config.FEATURES_PATH / f\"fold_{fold+1}_val\"\n",
    "        )\n",
    "        \n",
    "        # Calculate class weights\n",
    "        class_weights = train_ds.calculate_class_weights()\n",
    "        \n",
    "        # Initialize and train model\n",
    "        model = CloudDetectionModel()\n",
    "        val_history = model.fit(\n",
    "            train_ds.batch_generator,\n",
    "            val_ds.batch_generator,\n",
    "            class_weights\n",
    "        )\n",
    "        \n",
    "        # Get best validation metrics\n",
    "        best_metrics = max(val_history, key=lambda x: x['f1']) if val_history else None\n",
    "        if best_metrics:\n",
    "            print(f\"Fold {fold+1} best F1: {best_metrics['f1']:.4f}\")\n",
    "            cv_metrics.append(best_metrics)\n",
    "            \n",
    "            # Save fold-specific model\n",
    "            model.save(config.MODEL_PATH / f\"model_fold_{fold+1}.joblib\")\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    if cv_metrics:\n",
    "        avg_metrics = {\n",
    "            'accuracy': np.mean([m['accuracy'] for m in cv_metrics]),\n",
    "            'precision': np.mean([m['precision'] for m in cv_metrics]),\n",
    "            'recall': np.mean([m['recall'] for m in cv_metrics]),\n",
    "            'f1': np.mean([m['f1'] for m in cv_metrics])\n",
    "        }\n",
    "        \n",
    "        if 'roc_auc' in cv_metrics[0]:\n",
    "            avg_metrics['roc_auc'] = np.mean([m['roc_auc'] for m in cv_metrics])\n",
    "        \n",
    "        print(\"\\nCross-Validation Results:\")\n",
    "        print(f\"Avg Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Avg Precision: {avg_metrics['precision']:.4f}\")\n",
    "        print(f\"Avg Recall: {avg_metrics['recall']:.4f}\")\n",
    "        print(f\"Avg F1 Score: {avg_metrics['f1']:.4f}\")\n",
    "        if 'roc_auc' in avg_metrics:\n",
    "            print(f\"Avg ROC AUC: {avg_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Save CV metrics\n",
    "        all_cv_data = {\n",
    "            'folds': cv_metrics,\n",
    "            'average': avg_metrics\n",
    "        }\n",
    "        save_metrics(all_cv_data, 'cross_validation', config.OUTPUT_PATH)\n",
    "    \n",
    "    return cv_metrics\n",
    "\n",
    "# ============================================================================\n",
    "# Prediction and Visualization\n",
    "# ============================================================================\n",
    "def plot_image_and_mask(image, mask, output_path=None):\n",
    "    \"\"\"Plot satellite image with cloud mask overlay\"\"\"\n",
    "    # Convert image from CHW to HWC format if needed\n",
    "    if image.ndim == 3 and image.shape[0] in [3, 4]:\n",
    "        # Assuming image is (C, H, W) with C=4 (R,G,B,NIR)\n",
    "        # Create RGB image for visualization\n",
    "        rgb_image = np.transpose(image[:3], (1, 2, 0))\n",
    "    else:\n",
    "        rgb_image = image\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot original image (RGB bands)\n",
    "    axes[0].imshow(np.clip(rgb_image[:, :, :3], 0, 1))\n",
    "    axes[0].set_title(\"RGB Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Plot mask\n",
    "    axes[1].imshow(mask, cmap='gray')\n",
    "    axes[1].set_title(\"Cloud Mask\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Plot overlay\n",
    "    overlay = np.copy(rgb_image)\n",
    "    overlay_mask = np.zeros_like(rgb_image)\n",
    "    overlay_mask[:, :, 0] = mask * 0.7  # Red channel for clouds\n",
    "    \n",
    "    # Blend mask with image\n",
    "    alpha = 0.5\n",
    "    blended = (1-alpha) * rgb_image + alpha * overlay_mask\n",
    "    axes[2].imshow(np.clip(blended, 0, 1))\n",
    "    axes[2].set_title(\"Cloud Overlay\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Visualization saved to {output_path}\")\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def predict_full_image(model, image_path, output_path=None):\n",
    "    \"\"\"Predict cloud mask for a full image with tiling\"\"\"\n",
    "    # Load image\n",
    "    image = load_and_normalize_tiff(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Process image in tiles to save memory\n",
    "    img_tiles, coords = generate_tiles(image, config.TILE_SIZE)\n",
    "    h, w = image.shape[1], image.shape[2]\n",
    "    \n",
    "    # Initialize full mask\n",
    "    full_mask = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    # Process each tile\n",
    "    for tile, (y, x) in zip(img_tiles, coords):\n",
    "        # Extract features\n",
    "        indices = calculate_spectral_indices(tile)\n",
    "        features = extract_features(tile, indices)\n",
    "        \n",
    "        # Get probabilities\n",
    "        proba = model.predict_proba(features)\n",
    "        \n",
    "        # Reshape to tile shape\n",
    "        tile_mask = proba.reshape(config.TILE_SIZE, config.TILE_SIZE)\n",
    "        \n",
    "        # Place in full mask\n",
    "        full_mask[y:y+config.TILE_SIZE, x:x+config.TILE_SIZE] = tile_mask\n",
    "    \n",
    "    # Apply threshold and post-processing\n",
    "    binary_mask = (full_mask > config.PROBABILITY_THRESHOLD).astype(np.uint8)\n",
    "    \n",
    "    if config.POST_PROCESS:\n",
    "        # Remove small isolated pixels (noise)\n",
    "        binary_mask = ndimage.binary_opening(\n",
    "            binary_mask, \n",
    "            structure=np.ones((config.MORPHOLOGY_SIZE, config.MORPHOLOGY_SIZE))\n",
    "        )\n",
    "        \n",
    "        # Fill small holes\n",
    "        binary_mask = ndimage.binary_closing(\n",
    "            binary_mask, \n",
    "            structure=np.ones((config.MORPHOLOGY_SIZE, config.MORPHOLOGY_SIZE))\n",
    "        )\n",
    "    \n",
    "    # Visualize and save if needed\n",
    "    if output_path:\n",
    "        plot_image_and_mask(image, binary_mask, output_path)\n",
    "    \n",
    "    return binary_mask\n",
    "\n",
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "def main():\n",
    "    print(\"Cloud Detection Pipeline\")\n",
    "    print(\"=======================\")\n",
    "    \n",
    "    # 1. Find and list image pairs\n",
    "    print(\"Finding image pairs...\")\n",
    "    image_pairs = []\n",
    "    for category in ['cloud_free', 'partially_clouded', 'fully_clouded']:\n",
    "        img_dir = config.PROCESSED_DATA / \"data\" / category\n",
    "        mask_dir = config.PROCESSED_DATA / \"masks\" / category\n",
    "        \n",
    "        if not img_dir.exists() or not mask_dir.exists():\n",
    "            print(f\"Warning: Directory not found: {img_dir} or {mask_dir}\")\n",
    "            continue\n",
    "        \n",
    "        matched = [\n",
    "            (img_file, mask_dir / img_file.name, category)\n",
    "            for img_file in img_dir.glob('*.tif')\n",
    "            if (mask_dir / img_file.name).exists()\n",
    "        ]\n",
    "        \n",
    "        # Limit samples per category if needed\n",
    "        if config.SAMPLES_PER_CATEGORY > 0:\n",
    "            np.random.shuffle(matched)\n",
    "            matched = matched[:config.SAMPLES_PER_CATEGORY]\n",
    "        \n",
    "        image_pairs.extend(matched)\n",
    "    \n",
    "    print(f\"Found {len(image_pairs)} image pairs\")\n",
    "    if not image_pairs:\n",
    "        print(\"No image pairs found. Check your data paths.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Split data\n",
    "    print(\"\\nSplitting datasets...\")\n",
    "    train_pairs, val_test_pairs = train_test_split(\n",
    "        image_pairs,\n",
    "        test_size=config.VAL_SPLIT + config.TEST_SPLIT,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    val_pairs, test_pairs = train_test_split(\n",
    "        val_test_pairs,\n",
    "        test_size=config.TEST_SPLIT / (config.VAL_SPLIT + config.TEST_SPLIT),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {len(train_pairs)}, Validation: {len(val_pairs)}, Test: {len(test_pairs)}\")\n",
    "    \n",
    "    # 3. Create datasets with caching\n",
    "    train_ds = CloudDataset(\n",
    "        train_pairs,\n",
    "        mode='train',\n",
    "        feature_cache_dir=config.FEATURES_PATH / \"train\"\n",
    "    )\n",
    "    \n",
    "    val_ds = CloudDataset(\n",
    "        val_pairs,\n",
    "        mode='val',\n",
    "        feature_cache_dir=config.FEATURES_PATH / \"val\"\n",
    "    )\n",
    "    \n",
    "    test_ds = CloudDataset(\n",
    "        test_pairs,\n",
    "        mode='test',\n",
    "        feature_cache_dir=config.FEATURES_PATH / \"test\"\n",
    "    )\n",
    "    \n",
    "    # 4. Calculate class weights\n",
    "    print(\"\\nCalculating class weights...\")\n",
    "    class_weights = train_ds.calculate_class_weights()\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    \n",
    "    # 5. Run cross-validation or train single model\n",
    "    if config.CROSS_VAL_FOLDS > 1:\n",
    "        print(\"\\nRunning cross-validation...\")\n",
    "        cv_metrics = cross_validate(train_pairs, config.CROSS_VAL_FOLDS)\n",
    "        \n",
    "        # Load best model from cross-validation\n",
    "        best_model_path = config.MODEL_PATH / \"model_fold_1.joblib\"  # Default to first fold\n",
    "        if cv_metrics:\n",
    "            # Find best fold\n",
    "            best_f1 = 0\n",
    "            best_fold = 1\n",
    "            for fold, metrics in enumerate(cv_metrics, 1):\n",
    "                if metrics['f1'] > best_f1:\n",
    "                    best_f1 = metrics['f1']\n",
    "                    best_fold = fold\n",
    "            best_model_path = config.MODEL_PATH / f\"model_fold_{best_fold}.joblib\"\n",
    "        \n",
    "        print(f\"Loading best model from {best_model_path}\")\n",
    "        model = CloudDetectionModel.load(best_model_path)\n",
    "    else:\n",
    "        # Train a single model\n",
    "        print(\"\\nTraining model...\")\n",
    "        model = CloudDetectionModel()\n",
    "        model.fit(\n",
    "            train_ds.batch_generator,\n",
    "            val_ds.batch_generator,\n",
    "            class_weights\n",
    "        )\n",
    "    \n",
    "    # 6. Final evaluation on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_preds, test_true, test_proba = [], [], []\n",
    "    \n",
    "    for X_test, y_test in test_ds.batch_generator():\n",
    "        test_preds.extend(model.predict(X_test))\n",
    "        test_true.extend(y_test)\n",
    "        test_proba.extend(model.predict_proba(X_test))\n",
    "    \n",
    "    if test_true:\n",
    "        test_metrics = calculate_metrics(test_true, test_preds, test_proba)\n",
    "        print(\"\\nTest Set Results:\")\n",
    "        print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "        if 'roc_auc' in test_metrics:\n",
    "            print(f\"ROC AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Save test metrics\n",
    "        save_metrics(test_metrics, 'test', config.OUTPUT_PATH)\n",
    "    \n",
    "    # 7. Plot feature importance\n",
    "    plot_feature_importance(model, config.OUTPUT_PATH / \"logs\" / \"feature_importance.png\")\n",
    "    \n",
    "    # 8. Example predictions\n",
    "    print(\"\\nGenerating example predictions...\")\n",
    "    test_images = []\n",
    "    for _, _, category in test_pairs[:3]:  # Take one from each category if possible\n",
    "        img_dir = config.PROCESSED_DATA / \"data\" / category\n",
    "        for img_file in img_dir.glob('*.tif'):\n",
    "            test_images.append((img_file, category))\n",
    "            break\n",
    "    \n",
    "    for i, (img_path, category) in enumerate(test_images):\n",
    "        print(f\"Predicting for {img_path.name} ({category})...\")\n",
    "        output_path = config.OUTPUT_PATH / \"predictions\" / f\"pred_{img_path.stem}.png\"\n",
    "        _ = predict_full_image(model, img_path, output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
