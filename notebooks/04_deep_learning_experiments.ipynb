{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5082e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6504e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import random\n",
    "from models.unet import UNet\n",
    "from data_loader import prepare_datasets\n",
    "from evaluate import dice_coeff,iou_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79d09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    scaler = GradScaler()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc='Training') as pbar:\n",
    "        for i, (images, masks, _) in enumerate(train_loader):  # Discard label\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': loss.item() * accumulation_steps})\n",
    "\n",
    "    return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2aa711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device, threshold=0.5, visualize=False):\n",
    "    model.eval()\n",
    "    val_metrics = {\n",
    "        'loss': 0.0, 'dice': 0.0, 'iou': 0.0,\n",
    "        'precision': 0.0, 'recall': 0.0, 'f1': 0.0,\n",
    "        'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0\n",
    "    }\n",
    "    val_count = 0\n",
    "    per_image_metrics = []\n",
    "    fig = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks, _ in tqdm(val_loader, desc='Validation'):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            sigmoid_outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            val_metrics['loss'] += criterion(outputs, masks).item()\n",
    "            pred = (sigmoid_outputs > threshold).float()\n",
    "            \n",
    "            # Visualization\n",
    "            if visualize and fig is None and images.size(0) > 0:\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                axes[0].imshow(images[0].cpu().permute(1, 2, 0)[:, :, :3])\n",
    "                axes[0].set_title(\"Input\")\n",
    "                axes[1].imshow(masks[0].cpu().squeeze(), cmap='gray')\n",
    "                axes[1].set_title(\"Ground Truth\")\n",
    "                axes[2].imshow(pred[0].cpu().squeeze(), cmap='gray')\n",
    "                axes[2].set_title(f\"Prediction (th={threshold:.2f})\")\n",
    "                plt.show()\n",
    "            \n",
    "            # Metrics calculation\n",
    "            batch_metrics = {}\n",
    "            batch_metrics['dice'] = dice_coeff(sigmoid_outputs, masks).item()\n",
    "            batch_metrics['iou'] = iou_score(sigmoid_outputs, masks).item()\n",
    "            \n",
    "            pred_flat = pred.view(-1)\n",
    "            masks_flat = masks.view(-1)\n",
    "            \n",
    "            tp = torch.sum((pred_flat == 1) & (masks_flat == 1)).item()\n",
    "            fp = torch.sum((pred_flat == 1) & (masks_flat == 0)).item()\n",
    "            tn = torch.sum((pred_flat == 0) & (masks_flat == 0)).item()\n",
    "            fn = torch.sum((pred_flat == 0) & (masks_flat == 1)).item()\n",
    "            \n",
    "            batch_metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            batch_metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            batch_metrics['f1'] = 2 * (batch_metrics['precision'] * batch_metrics['recall']) / (\n",
    "                batch_metrics['precision'] + batch_metrics['recall']) if (\n",
    "                batch_metrics['precision'] + batch_metrics['recall']) > 0 else 0\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                per_image_metrics.append({\n",
    "                    'dice': dice_coeff(sigmoid_outputs[i:i+1], masks[i:i+1]).item(),\n",
    "                    'iou': iou_score(sigmoid_outputs[i:i+1], masks[i:i+1]).item(),\n",
    "                })\n",
    "            \n",
    "            for k in batch_metrics:\n",
    "                val_metrics[k] += batch_metrics[k]\n",
    "            \n",
    "            val_metrics['tp'] += tp\n",
    "            val_metrics['fp'] += fp\n",
    "            val_metrics['tn'] += tn\n",
    "            val_metrics['fn'] += fn\n",
    "            val_count += 1\n",
    "    \n",
    "    # Calculate averages\n",
    "    for k in ['loss', 'dice', 'iou', 'precision', 'recall', 'f1']:\n",
    "        val_metrics[k] /= max(1, val_count)\n",
    "    \n",
    "    # Global metrics\n",
    "    total_tp = val_metrics['tp']\n",
    "    total_fp = val_metrics['fp']\n",
    "    total_fn = val_metrics['fn']\n",
    "    \n",
    "    val_metrics['global_precision'] = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    val_metrics['global_recall'] = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    val_metrics['global_f1'] = 2 * (val_metrics['global_precision'] * val_metrics['global_recall']) / (\n",
    "        val_metrics['global_precision'] + val_metrics['global_recall']) if (\n",
    "        val_metrics['global_precision'] + val_metrics['global_recall']) > 0 else 0\n",
    "    \n",
    "    # Standard deviations\n",
    "    dice_values = [m['dice'] for m in per_image_metrics]\n",
    "    iou_values = [m['iou'] for m in per_image_metrics]\n",
    "    val_metrics['dice_std'] = np.std(dice_values) if dice_values else 0\n",
    "    val_metrics['iou_std'] = np.std(iou_values) if iou_values else 0\n",
    "    \n",
    "    return val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3588f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, val_loader, device, n_thresholds=20):\n",
    "    model.eval()\n",
    "    thresholds = np.linspace(0.1, 0.9, n_thresholds)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for th in thresholds:\n",
    "            val_metrics = validate(model, val_loader, nn.BCEWithLogitsLoss(), device, threshold=th, visualize=False)\n",
    "            if val_metrics['global_f1'] > best_f1:\n",
    "                best_f1 = val_metrics['global_f1']\n",
    "                best_threshold = th\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d3146c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight(dataset):\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        # Handle both cases where dataset returns (image, mask) or (image, mask, label)\n",
    "        if len(item) == 3:\n",
    "            _, mask, _ = item  # Discard image and label\n",
    "        else:\n",
    "            _, mask = item  # Discard image\n",
    "        \n",
    "        # Assuming mask is shape (1, H, W) or (H, W)\n",
    "        mask = mask.float()\n",
    "        total_pos += mask.sum().item()\n",
    "        total_neg += (1 - mask).sum().item()\n",
    "\n",
    "    return total_neg / (total_pos + 1e-8)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "\n",
    "        intersection = (probs * targets).sum(dim=(1,2,3))\n",
    "        union = probs.sum(dim=(1,2,3)) + targets.sum(dim=(1,2,3))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b09d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, device):\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    with open('best_threshold.txt', 'r') as f:\n",
    "        best_threshold = float(f.read())\n",
    "    \n",
    "    test_metrics = validate(model, test_loader, criterion, device, best_threshold, visualize=True)\n",
    "    \n",
    "    print(\"\\n--- Test Metrics ---\")\n",
    "    print(f\"Validation Threshold Used: {best_threshold:.2f}\")\n",
    "    print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "    print(f\"Test Dice: {test_metrics['dice']:.4f}±{test_metrics['dice_std']:.4f}\")\n",
    "    print(f\"Test IoU: {test_metrics['iou']:.4f}±{test_metrics['iou_std']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['global_precision']:.4f} | Recall: {test_metrics['global_recall']:.4f} | F1: {test_metrics['global_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d341976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size = 4\n",
    "    accumulation_steps = 2  # Effective batch size = 8\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    patience = 10\n",
    "\n",
    "    # Prepare data\n",
    "    try:\n",
    "        train_set, val_set, test_set = prepare_datasets('../data/processed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing datasets: {e}\")\n",
    "        return\n",
    "    \n",
    "    pos_weight_value = compute_pos_weight(train_set)\n",
    "    pos_weight = torch.tensor([pos_weight_value]).to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False,\n",
    "                           num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize model\n",
    "    model = UNet(n_channels=4, n_classes=1).to(device)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    dice = DiceLoss()\n",
    "    \n",
    "    def combined_loss_fn(logits, targets):\n",
    "        return bce(logits, targets) + dice(logits, targets)\n",
    "    \n",
    "    criterion = combined_loss_fn\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5)\n",
    "\n",
    "    best_dice = 0.0\n",
    "    best_threshold = 0.5\n",
    "    no_improve = 0\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            avg_train_loss = train_epoch(model, train_loader, optimizer, criterion, \n",
    "                                       device, accumulation_steps)\n",
    "            \n",
    "            # Validation with dynamic threshold\n",
    "            threshold = find_optimal_threshold(model, val_loader, device)\n",
    "            metrics = validate(model, val_loader, criterion, device, \n",
    "                             threshold, visualize=(epoch % 10 == 0))\n",
    "            \n",
    "            print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {metrics['loss']:.4f}\")\n",
    "            print(f\"Val Dice: {metrics['dice']:.4f}±{metrics['dice_std']:.4f} | Threshold: {threshold:.2f}\")\n",
    "            print(f\"Precision: {metrics['global_precision']:.4f} | Recall: {metrics['global_recall']:.4f} | F1: {metrics['global_f1']:.4f}\")\n",
    "\n",
    "            # Early stopping and model saving\n",
    "            if metrics['dice'] > best_dice:\n",
    "                best_dice = metrics['dice']\n",
    "                best_threshold = threshold\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                with open('best_threshold.txt', 'w') as f:\n",
    "                    f.write(str(best_threshold))\n",
    "                no_improve = 0\n",
    "                print(f\"New best model! Val Dice: {best_dice:.4f}\")\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"No improvement for {patience} epochs. Early stopping!\")\n",
    "                    break\n",
    "\n",
    "            scheduler.step(metrics['dice'])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "        \n",
    "    test_model(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cc41d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train: 8458, Val: 1058, Test: 1058\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 42/2115 [00:20<16:41,  2.07it/s, loss=0.562]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user.\n",
      "\n",
      "Evaluating on test set...\n",
      "Critical error: [Errno 2] No such file or directory: 'best_model.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "try:\n",
    "    train_model()\n",
    "except Exception as e:\n",
    "    print(f\"Critical error: {e}\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ce96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTONLY = False\n",
    "if TESTONLY:\n",
    "    batch_size = 4\n",
    "\n",
    "    try:\n",
    "        train_set, val_set, test_set = prepare_datasets('../data/processed')\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing datasets: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    pos_weight_value = compute_pos_weight(train_set)\n",
    "    pos_weight = torch.tensor([pos_weight_value]).to(device)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = UNet(n_channels=4, n_classes=1).to(device)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    dice = DiceLoss()\n",
    "\n",
    "    def combined_loss_fn(logits, targets):\n",
    "        return bce(logits, targets) + dice(logits, targets)\n",
    "\n",
    "    criterion = combined_loss_fn\n",
    "\n",
    "    test_model(model, test_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
